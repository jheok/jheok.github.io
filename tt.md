진행했던 프로젝트 두 가지에 대하여 활용 기술, 본인의 역할, 진행 방식, 문제 해결 경험 등을 포함하여 구체적으로 설명해 주세요.
- 웹기반 빅데이터 분석 플랫폼 (레거시) 유지보수
  - 대용량 데이터 처리를 위한 파티셔닝 최적화 (단순 분석의 슬로우 쿼리를 대용량 분석 쿼리로 개선)



## 프로토콜의 흐름

1. 사용자는 내가 등록한 연결정보에 존재하는 데이터(시구, 시군구, 읍면동, 리, 본번, 부번, 지목부호, 등 법정동을 기반한 데이터)에 해당하는 폴리곤을 화면상에서 보고싶다
2. 데이터를 등록하게 되면 모델 정보(파일 혹은 db 를 이용한 스키마 정의)가 생성된다 
3. 등록된 데이터를 이용하여 만들어진 모델에 대한 DSL 명령어를 작성 후 백엔드에 dsl 쿼리를 전달한다 
4. 서버는 DSL 쿼리를 파싱해 파싱된 파라미터들로 pyspark udf 를 개발하여 사용자가 원하는 데이터를 프론트에 전달한다 (담당 : 개선 및 추가 개발) 
5. 프론트는 사용자가 원하는 폴리곤을 지도에 표현한다

## 당시 상황
100GB 이상의 행정동, 법정동에 대한 기준이 모호한 지도 데이터와 폴리곤 데이터를 함유하고 있는 .csv 파일을 단순 분할하여 각 spark node 마다 데이터를 보유하고 있다 
사용자가 등록한 데이터에 일치하는 폴리곤을 추출하여 사용자 데이터에 polygon 을 join 하여 다시 전달하고 있다

## 상황 파악
당시 해당 DSL 명령어는 무거운 폴리곤 데이터를 가지고 있기 때문에 조회를 하는게 당연히 느린 DSL 명령어라고 생각하여 단순 몇건에 대한 데이터에 polygon 데이터를 
join을 해주는 명령어로써 사용되고 있었다
그래서 나도 당시 처음에는 단순히 행정동, 법정동에 대한 기준이 모호한 지도 데이터를 최신의 데이터로 업데이트 하면서 기준만 재구축하려고 했었다
하지만 spark 를 사용하고 있는데 데이터 분석이 느린지에 대해서 이해를 할 수 없었다.

## 상황 분석
"100GB 이상의 행정동, 법정동에 대한 기준이 모호한 지도 데이터와 폴리곤 데이터를 함유하고 있는 .csv 파일을 단순 분할하여 각 spark node 마다 데이터를 보유"
내가 내렸던 상황에 대해 좀더 분석을 해보았다 특히 윗 문장을 분석했을때 가장 문제가 되는 2가지 부분을 발견했는데

첫번째로는 100GB 이상의 행정동, 법정동에 대한 기준이 모호한 지도 데이터와 폴리곤 데이터를 함유하고 있는 .csv 파일
두번째로는 .csv 파일을 단순 분할

이 두가지가 이 상황을 해결해 줄 수 있는 키 포인트라 생각했다

## 문제 해결

0. `100GB 이상의 행정동, 법정동에 대한 기준이 모호한 지도 데이터와 폴리곤 데이터를 함유하고 있는 .csv 파일` -> `100GB 이상의 법정동에 기반한 최신의 지도 데이터와 폴리곤 데이터를 함유하고 있는 .csv 파일`
일단 첫번째의 문제를 해결하기 전 나는 이 오래된 데이터를 최신의 데이터로 업데이트 하며 기준을 명확히 하려고 했었다
왜냐하면 오래된 데이터의 출처는 아무도 모르고(해당 데이터에 대한 문서가 없었다) 최초에 해당 프로젝트의 DSL 명령어를 개발했을때 생성해 놨던 데이터 였다
그래서 데이터 분석으로 추출된 데이터가 현재와는 일치하지 않은 데이터 였기도 했다
이 문제를 해결하기 위해서 데이터에 대한 정보도 따로 문서에 정리 해놓았는데 당시의 정보이다

---------------------------

### 0-1. 사전데이터의 과정

#### download

- 다운로드 주소: http://openapi.nsdi.go.kr/nsdi/#
    - 연속지적도형정보(SHP)파일 다운로드
- 좌표계: EPSG:5174
- 데이터 인코딩: cp949
- 데이터기준일자 : 2021-05-01

#### toWGS84 데이터

- 변경방법:         QGIS 3.18.0 활용
- 좌표계:           EPSG:4326
- 데이터 인코딩:    utf-8

#### 0-2. 연속지적도형 정보조회 서비스 컬럼 정의 (default 컬럼)

| 항목명(영문) | 항목명(국문)   | 항목크기 | 항목구분 | 샘플데이터                  | 항목설명                                                     |
| ------------ | -------------- | -------- | -------- | --------------------------- | ------------------------------------------------------------ |
| A0           | 원천도형ID     | 10       | 1        | 56498                       | 개방DB에서 정의한 연속지적도의 도형 ID                       |
| A1           | 고유번호       | 19       | 1        | 411731020010883000          | 각 필지를 서로 구별하기 위하여 필지마다 붙이는 고유한 번호   |
| A2           | 법정동 코드    | 10       | 1        | 4117310200                  | 토지가 소재한 행정구역코드(법정동코드) 10자리                |
| A3           | 법정동명       | 300      | 1        | 경기도 안양시 동안구 관양동 | 토지가 소재한 소재지의 행정구역 명칭(법정동명)               |
| A4           | 지번           | 8        | 1        | 1454-1<br/>883              | 필지에 부여하여 지적공부에 등록한 번호. 지번본번과 지번부번으로   구성 |
| A5           | 지번지목부호   | 200      | 1        | 1451-1대<br/>883장          | 연속지적도의 각 필지에 표시된 지번+지목부호                  |
| A6           | 데이터기준일자 | 36       | 1        | 2014-10-10                  | 데이터 작성 기준일자                                         |

- 항목구분 : 필수(1), 옵션(0), 1건 이상 복수건(1 ... n), 0건 또는 복수건 (0...n)


#### 0-3. 변경 컬럼명 정의

| 항목명(영문) | 변경 항목명(영문) | 항목명(국문) | 예시                        |
| ------------ | ----------------- | ------------ | --------------------------- |
| WKT          | WKT               | X            | polygon~~~~                 |
| A0           | SGG_OID           | 원천도형ID   | 56498                       |
| A1           | PNU               | 고유번호     | 411731020010883000          |
| A2           | BJD_CD            | 법정동 코드  | 4117310200                  |
| A3           | BJD_NM            | 법정동명     | 경기도 안양시 동안구 관양동 |
|              | SI_NM             | 시도         | 경기도                      |
|              | SGG_NM            | 시군구       | 안양시 동안구               |
|              | EMD_NM            | 읍면동       | 관양동                      |
|              | RI_NM             | 리           | 금남리                      |
| A4           | BONBUN            | 본번         | 1454 or 산1454              |
|              | BUBON             | 부번         | 12                          |
| A5           | JIMOK             | 지목부호     | 대                          |



#### 0-4. 서울 최종 데이터 예시


| WKT                                                          | SGG_OID | PNU                 | BJD_CD     | BJD_NM                   | SI_NM      | SGG_NM | EMD_NM | RI_NM | BONBUN | BUBON | JIMOK |
| ------------------------------------------------------------ | ------- | ------------------- | ---------- | ------------------------ | ---------- | ------ | ------ | ----- | ------ | ----- | ----- |
| MULTIPOLYGON (((126.968673524572 37.5904663101548...)))      | 206422  | 1111010100100009980 | 1111010100 | 서울특별시 종로구 청운동 | 서울특별시 | 종로구 | 청운동 |       | 1      |       | 대    |
| MULTIPOLYGON (((126.968631991748 37.5904445217881...)))      | 164661  | 1111010100100009980 | 1111010100 | 서울특별시 종로구 청운동 | 서울특별시 | 종로구 | 청운동 |       | 1      | 3     | 대    |
| MULTIPOLYGON (((126.961894185078 37.578484470874 .......)))  | 169625  | 1111011500200009980 | 1111011500 | 서울특별시 종로구 사직동 | 서울특별시 | 종로구 | 사직동 |       | 산1    | 25    | 도    |
| MULTIPOLYGON (((126.961607472317 37.5781525859572 .........))) | 169622  | 1111011500200009980 | 1111011500 | 서울특별시 종로구 사직동 | 서울특별시 | 종로구 | 사직동 |       | 산1    |       | 공    |



#### 0-5. 데이터 만드는 과정

- 다운로드 주소에서 shp 파일 시도 구분으로 전부 다운로드
- qgis에서 shp 파일 불러오기
- 불러온 파일 객체식별 => 정보 => 속성 원본 =>CP949
- 원하는 좌표계로 바꾸기(포맷 =  csv) = > export => wgs84로, 인코딩 utf8
- geometry를 AS_WKT
- 생성된 레이어의 좌표가 맞는지 확인하기 => 아래 좌표(경도, 위도)나옴
- 좌표로 구글, 네이버, 카카오 등 맵에서 직접 비교하면서 확인하기
- default 컬럼으로 구성 되어 있는 csv 파일 확인하기
- csv 파일의 컬럼을 원하는 구성에 맞춰서 파싱하여 새로 저장
- csv파일을 원하는 파일포멧에 맞게 저장

- 이를 시구, 시군구, 읍면동, 리, 본번, 부번, 지목부호, 등 법정동을 기반한 데이터에 전부 적용 한다


---------------------------

당시 무식하게 데이터를 만들었는데 그 이유는 `0-5. 데이터 만드는 과정`을 자동화 하게 되면 qgis 를 사용할 수 없었으며 노력 대비 효율이 현저히 떨어졌기 때문이다
실제도 스크립트를 만들어 자동화를 해봤으나 QGIS 를 사용하여 데이터를 추출하고 가공하는 과정에 비하면 몇배 이상의 시간이 들어가서 비효율적일수 밖에 없었다

이렇게 진행을 하게 되면 `연속지적도형 정보조회 서비스 컬럼 정의`에서 지원하는 데이터를 내가 원하는 형태의 데이터로 변환을 할 수 있었는데 `변경 컬럼명 정의` 가 바로 그 형태이다 
이렇게 가장 최초에 진행 하려 했던 행정동, 법정동에 대한 기준이 모호한 지도 데이터를 최신의 법정동에 기반한 지적도 데이터로 업데이트를 만족했다

하지만 이는 데이터를 업데이트한 과정이며 여전히 대용량 분석에는 적합하지 않았다


1. `100GB 이상의 행정동, 법정동에 대한 기준이 모호한 데이터를 함유하고 있는 .csv 파일` -> `100GB 이하의 법정동에 기반한 최신의 데이터를 함유하고 있는 .parquet 파일`
현재 이 개선사항을 진행하는데 있어서 제약사항이 존재했는데 파일시스템 기반으로 개선을 해야하는 것이었다
이유는 업체에 납품되어 있는 환경들이 전부 파일시스템 기반이며 db 기반으로 변경을 하게 되면 소규모로 진행했던 개선사항의 목적이 상실되었기 때문이다
그리고 파일시스템 기반으로도 충분히 성능을 향상 시킬수 있다고 봤기 때문이다

이제 개선사항의 최초 목적을 이루었으니 문제 해결을 위한 작업이 필요했다

그중 csv 파일을 대용량으로 사용하고 있던 것이 가장 눈에 들어왔다
이유는 csv 파일은 row 기반의 데이터이며 이를 활용해 사용자가 원하는 데이터를 추출하는 방식은 인덱스가 존재하지 않는다면 모든 데이터를 로드해야 원하는 레코드를 찾을 수 있었기 때문이다
spark 가 inmemory 방식으로 동작한다 하여도 많은 디스크 IO가 발생하고, 성능 저하가 되기 때문에 내가 원하는 방향에는 전혀 어울리지 않는 파일 포맷 이었다

이를 해결하기 위해 column 기반의 데이터 포맷을 찾았다
이유는 column 별로 데이터가 저장되어 있기 때문에, 데이터 분석시 필요한 column 만을 로드 하여 디스크 IO를 줄이고
모든 column 을 가져올 필요가 없고 DSL 쿼리에서 요구 되는 column 만 읽을 수 있기 때문에 속도가 row 기반 처리 방식에 비해서 높은 성능을 가질수 있기 때문이다

마침 내가 생각했던을 spark 는 이미 반영을 하고 있었다. 특히 .parquet 포맷형태를 지원하고 있었으며

이를 사용함으로써 원본 데이터의 압축률도 좋아지고 spark 를 사용함에 있어 능률도 향상시킬수 있었다.


---------------------------

### 1. 시도, 시군구, 읍면동, .... -> WKT 정보

#### 1-1. 시도 컬럼을 기준으로 파일 구성

```
.
├--- 서울
|   └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   └---- part-00001-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   └---- part-00002-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
├--- 세종
|   └---- part-00000-5644a4cb-a03b-4201-9f25-54151a9987ac-c000.snappy.parquet
.
.

```

#### 1-1-1. 결과 (13~15초)

- 매번 전체 데이터를 읽음으로 인해 오랜시간이 걸림
- 하지만 기존 아예 동작이 안된 대용량 분석 DSL 쿼리가 동작되기 시작함


2. `.csv 파일을 단순 분할` -> `.parquert 파일을 명확한 기준을 가지고 partitioning`
당시 csv 파일을 단순 분할하여 spark 의 분산처리를 이용하려 했던것 같았는데 이는 제대로 spark 를 활용하지 못한것이라 생각했다
물론 내부 데이터에 명확한 기준이 있어 그를 활용해 파일을 분할 했다면 이점을 살릴수 있었겠으나 이는 진행되지 않고 그저 데이터 덩어리로만 남겨져 있어 이를 개선하기로 생각했다
처음으로 데이터를 분할하기에 앞서서 명확한 기준인 시도, 시군구, 읍면동, 리 로 기준을 잡았다
이를 이용해 각 기준점을 컬럼으로 생각하면 기준에 대한 데이터만 불러와 필요없는 데이터의 로드는 줄일수 있지않을까 하고 생각했었으며
특히 spark 의 parquet partitioning 을 이용하여 parquet 파일을 사용하는데 있어 최대의 이점을 얻을수 있었다



#### 1-2. 시도, 시군구, 읍면동, 리 를 기준으로 파일구성

```
.
└--- 서울
|   ├---- 종로구
|   |     └---- 익선동
|   |          └-----NULL
|   |               └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   ├---- 강동구
|   |     └---- 익선동
|   |          └-----NULL
|   |               └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   ├---- 송파구
|   ├---- 서초구
|   ....
├--- 세종
....

```

#### 1-2-1. 결과 (3~4초)

- 시도, 시군구, 읍면동, 리를 기준으로 나눠 파일트리 구성
- 데이터값이 잘못됐을 경우 혹은 전체를 포함하고 싶은경우에는 다시 제대로 수정해줘야함
- 기존 시도 컬럼을 기준으로 파일 구성한 결과보다 2배 이상 시간 단축



#### 1-3. 시도, 시군구, 읍면동, 리 를 기준으로 parquet partitioning

```
.
└--- SI_NM=서울특별시
|   ├---- SGG=종로구
|   |     └---- 익선동
|   |          └-----NULL
|   |               └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   ├---- 강동구
|   |     └---- 익선동
|   |          └-----NULL
|   |               └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   ├---- 송파구
|   ├---- 서초구
|   ....
├--- sejong
....

```

#### 1-3-1. 결과 (1~2초)

- spark를 이용하여 parquet파일을 partitioning 함
- 데이터값이 잘못됐을 경우 혹은 전체를 포함하고 싶은경우 * 를 이용하여 그부분을 read할 수 있음
- 시도, 시군구, 읍면동, 리 를 기준으로 파일구성한 경우보다 2배 이상 시간 단축


#### 결론

- 아예 동작하지 않던 대용량 분석 DSL 쿼리 -> 1~2 초 대의 DSL 쿼리 로 개선
- 이후 프론트 팀에서 Map 관련 명령어를 이용한 팀 구축
