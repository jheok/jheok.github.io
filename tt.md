진행했던 프로젝트 두 가지에 대하여 활용 기술, 본인의 역할, 진행 방식, 문제 해결 경험 등을 포함하여 구체적으로 설명해 주세요.
- 웹기반 빅데이터 분석 플랫폼 (레거시) 유지보수
  - 대용량 데이터 처리를 위한 파티셔닝 최적화 (단순 분석의 슬로우 쿼리를 대용량 분석 쿼리로 개선)



## 프로토콜의 흐름

1. 사용자는 내가 등록한 연결정보에 존재하는 데이터(시구, 시군구, 읍면동, 리, 본번, 부번, 지목부호, 등 법정동을 기반한 데이터)에 해당하는 폴리곤을 화면상에서 보고싶다
2. 데이터를 등록하게 되면 모델 정보(파일 혹은 db 를 이용한 스키마 정의)가 생성된다 
3. 등록된 데이터를 이용하여 만들어진 모델에 대한 DSL 명령어를 작성 후 백엔드에 dsl 쿼리를 전달한다 
4. 서버는 DSL 쿼리를 파싱해 파싱된 파라미터들로 pyspark udf 를 개발하여 사용자가 원하는 데이터를 프론트에 전달한다 (담당 / 개선 및 추가 개발) 
5. 프론트는 사용자가 원하는 폴리곤을 지도에 표현한다

## 당시 상황
100GB 이상의 행정동, 법정동에 대한 기준이 모호한 지도 데이터와 폴리곤 데이터를 함유하고 있는 .csv 파일을 단순 분할하여 각 spark node 마다 데이터를 보유하고 있다 
사용자가 등록한 데이터에 일치하는 폴리곤을 추출하여 사용자 데이터에 polygon 을 join 하여 다시 전달하고 있다

## 상황 파악
당시 해당 DSL 명령어는 무거운 폴리곤 데이터를 가지고 있기 때문에 조회를 하는게 당연히 느린 DSL 명령어라고 생각하여 단순 몇건에 대한 데이터에 polygon 데이터를 
join을 해주는 명령어로써 사용되고 있었다
그래서 나도 당시 처음에는 단순히 행정동, 법정동에 대한 기준이 모호한 지도 데이터를 최신의 데이터로 업데이트 하면서 기준만 재구축하려고 했었다
하지만 spark 를 사용하고 있는데 데이터 분석이 느린지에 대해서 이해를 할 수 없었다.

## 상황 분석
"100GB 이상의 행정동, 법정동에 대한 기준이 모호한 지도 데이터와 폴리곤 데이터를 함유하고 있는 .csv 파일을 단순 분할하여 각 spark node 마다 데이터를 보유"
윗 문장을 분석했을때 가장 문제가 되는 2가지 부분을 발견했는데

첫번째로는 100GB 이상의 행정동, 법정동에 대한 기준이 모호한 지도 데이터와 폴리곤 데이터를 함유하고 있는 .csv 파일
두번째로는 .csv 파일을 단순 분할

이 두가지가 이 상황을 해결해 줄 수 있는 키 포인트라 생각했다

## 문제 해결

0.
일단 첫번째의 문제를 해결하기 전 나는 이 오래된 데이터를 최신의 데이터로 업데이트 하며 기준을 명확히 하려고 했었다
왜냐하면 오래된 데이터의 출처는 아무도 모르고 최초에 해당 프로젝트의 DSL 명령어를 개발했을때 생성해 놨던 데이터 였다
그래서 데이터 분석으로 추출된 데이터가 현재와는 일치하지 않은 데이터 였기도 했다
이 문제를 해결하기 위해서 데이터에 대한 정보도 따로 docs에 정리 해놓았는데 당시의 정보이다

---------------------------

### 0. 사전준비

#### 0-1. 사전데이터의 과정

#### download

- 다운로드 주소: http://openapi.nsdi.go.kr/nsdi/#
    - 연속지적도형정보(SHP)파일 다운로드
- 좌표계: EPSG:5174
- 데이터 인코딩: cp949
- 데이터기준일자 : 2021-05-01

#### toWGS84 데이터

- 변경방법:         QGIS 3.18.0 활용
- 좌표계:           EPSG:4326
- 데이터 인코딩:    utf-8

#### 0-2. 연속지적도형 정보조회 서비스 컬럼 정의 (default 컬럼)

| 항목명(영문) | 항목명(국문)   | 항목크기 | 항목구분 | 샘플데이터                  | 항목설명                                                     |
| ------------ | -------------- | -------- | -------- | --------------------------- | ------------------------------------------------------------ |
| A0           | 원천도형ID     | 10       | 1        | 56498                       | 개방DB에서 정의한 연속지적도의 도형 ID                       |
| A1           | 고유번호       | 19       | 1        | 411731020010883000          | 각 필지를 서로 구별하기 위하여 필지마다 붙이는 고유한 번호   |
| A2           | 법정동 코드    | 10       | 1        | 4117310200                  | 토지가 소재한 행정구역코드(법정동코드) 10자리                |
| A3           | 법정동명       | 300      | 1        | 경기도 안양시 동안구 관양동 | 토지가 소재한 소재지의 행정구역 명칭(법정동명)               |
| A4           | 지번           | 8        | 1        | 1454-1<br/>883              | 필지에 부여하여 지적공부에 등록한 번호. 지번본번과 지번부번으로   구성 |
| A5           | 지번지목부호   | 200      | 1        | 1451-1대<br/>883장          | 연속지적도의 각 필지에 표시된 지번+지목부호                  |
| A6           | 데이터기준일자 | 36       | 1        | 2014-10-10                  | 데이터 작성 기준일자                                         |

- 항목구분 : 필수(1), 옵션(0), 1건 이상 복수건(1 ... n), 0건 또는 복수건 (0...n)


#### 0-3. 변경 컬럼명 정의

| 항목명(영문) | 변경 항목명(영문) | 항목명(국문) | 예시                        |
| ------------ | ----------------- | ------------ | --------------------------- |
| WKT          | WKT               | X            | polygon~~~~                 |
| A0           | SGG_OID           | 원천도형ID   | 56498                       |
| A1           | PNU               | 고유번호     | 411731020010883000          |
| A2           | BJD_CD            | 법정동 코드  | 4117310200                  |
| A3           | BJD_NM            | 법정동명     | 경기도 안양시 동안구 관양동 |
|              | SI_NM             | 시도         | 경기도                      |
|              | SGG_NM            | 시군구       | 안양시 동안구               |
|              | EMD_NM            | 읍면동       | 관양동                      |
|              | RI_NM             | 리           | 금남리                      |
| A4           | BONBUN            | 본번         | 1454 or 산1454              |
|              | BUBON             | 부번         | 12                          |
| A5           | JIMOK             | 지목부호     | 대                          |



#### 0-4. 서울 최종 데이터 예시


| WKT                                                          | SGG_OID | PNU                 | BJD_CD     | BJD_NM                   | SI_NM      | SGG_NM | EMD_NM | RI_NM | BONBUN | BUBON | JIMOK |
| ------------------------------------------------------------ | ------- | ------------------- | ---------- | ------------------------ | ---------- | ------ | ------ | ----- | ------ | ----- | ----- |
| MULTIPOLYGON (((126.968673524572 37.5904663101548...)))      | 206422  | 1111010100100009980 | 1111010100 | 서울특별시 종로구 청운동 | 서울특별시 | 종로구 | 청운동 |       | 1      |       | 대    |
| MULTIPOLYGON (((126.968631991748 37.5904445217881...)))      | 164661  | 1111010100100009980 | 1111010100 | 서울특별시 종로구 청운동 | 서울특별시 | 종로구 | 청운동 |       | 1      | 3     | 대    |
| MULTIPOLYGON (((126.961894185078 37.578484470874 .......)))  | 169625  | 1111011500200009980 | 1111011500 | 서울특별시 종로구 사직동 | 서울특별시 | 종로구 | 사직동 |       | 산1    | 25    | 도    |
| MULTIPOLYGON (((126.961607472317 37.5781525859572 .........))) | 169622  | 1111011500200009980 | 1111011500 | 서울특별시 종로구 사직동 | 서울특별시 | 종로구 | 사직동 |       | 산1    |       | 공    |



#### 0-5. 데이터 만드는 과정

- 다운로드 주소에서 shp 파일 시도 구분으로 전부 다운로드
- qgis에서 shp 파일 불러오기
- 불러온 파일 객체식별 => 정보 => 속성 원본 =>CP949
- 원하는 좌표계로 바꾸기(포맷 =  csv) = > export => wgs84로, 인코딩 utf8
- geometry를 AS_WKT
- 생성된 레이어의 좌표가 맞는지 확인하기 => 아래 좌표(경도, 위도)나옴
- 좌표로 구글, 네이버, 카카오 등 맵에서 직접 비교하면서 확인하기
- default 컬럼으로 구성 되어 있는 csv 파일 확인하기
- csv 파일의 컬럼을 원하는 구성에 맞춰서 파싱하여 새로 저장
- csv파일을 원하는 파일포멧에 맞게 저장

- 이를 시구, 시군구, 읍면동, 리, 본번, 부번, 지목부호, 등 법정동을 기반한 데이터 에 전부 적용한다


---------------------------

당시 무식하게 데이터를 만들었는데 그 이유는 `0-5. 데이터 만드는 과정`을 자동화 하게 되면 qgis 를 사용할 수 없었으며 노력 대비 효율이 현저히 떨어졌기 때문이다
당시 스크립트를 만들어 자동화를 해봤으나 QGIS 를 사용하여 데이터를 추출하고 가공하는 과정에 비하면 몇배 이상의 시간이 들어가서 비 효율적일수 밖에 없었다

이렇게 진행을 하게 되면 `연속지적도형 정보조회 서비스 컬럼 정의`에서 지원하는 데이터를 내가 원하는 형태의 데이터로 변환을 할 수 있었는데 `변경 컬럼명 정의` 가 바로 그 형태이다 
이렇게 가장 최초에 진행 하려 했던 행정동, 법정동에 대한 기준이 모호한 지도 데이터를 최신의 데이터로 업데이트를 만족했다

하지만 이는 데이터를 업데이트한 과정이며 여전히 대용량 분석에는 적합하지 않았다


1.
현재 이 개선사항을 진행하는데 있어서 제약사항이 존재했는데 파일시스템 기반으로 개선을 해야하는 것이었다
첫번째로는 100GB 이상의 행정동, 법정동에 대한 기준이 모호한 지도 데이터와 폴리곤 데이터를 함유하고 있는 .csv 파일







### 1. 시도, 시군구, 읍면동, .... -> WKT 정보

##### 1-1. 시도 컬럼을 기준으로 파일 구성

```
.
├--- 서울
|   └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   └---- part-00001-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   └---- part-00002-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
├--- 세종
|   └---- part-00000-5644a4cb-a03b-4201-9f25-54151a9987ac-c000.snappy.parquet
.
.

```

##### 1-1-1. 결과 (13~15초)

- 매번 전체 데이터를 읽어옴으로 인해 오랜시간이 걸림



##### 1-2. 시도, 시군구, 읍면동, 리 를 기준으로 파일구성

```
.
└--- 서울
|   ├---- 종로구
|   |     └---- 익선동
|   |          └-----NULL
|   |               └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   ├---- 강동구
|   |     └---- 익선동
|   |          └-----NULL
|   |               └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   ├---- 송파구
|   ├---- 서초구
|   ....
├--- 세종
....

```

##### 1-2-1. 결과 (3~4초)

- 시도, 시군구, 읍면동, 리를 기준으로 나눠 파일트리 구성
- 데이터값이 잘못됐을 경우 혹은 전체를 포함하고 싶은경우에는 다시 제대로 수정해줘야함
- 기존 시도 컬럼을 기준으로 파일 구성한 결과보다 2배 이상 시간 단축



##### 1-3. 시도, 시군구, 읍면동, 리 를 기준으로 parquet partitioning

```
.
└--- SI_NM=서울특별시
|   ├---- SGG=종로구
|   |     └---- 익선동
|   |          └-----NULL
|   |               └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   ├---- 강동구
|   |     └---- 익선동
|   |          └-----NULL
|   |               └---- part-00000-ae704512-c6db-4f85-81e1-c02458d94ec6-c000.snappy.parquet
|   ├---- 송파구
|   ├---- 서초구
|   ....
├--- sejong
....

```

##### 1-3-1. 결과 (1~2초)

- spark를 이용하여 parquet파일을 partitioning 함
- 데이터값이 잘못됐을 경우 혹은 전체를 포함하고 싶은경우 * 를 이용하여 그부분을 read할 수 있음
- 시도, 시군구, 읍면동, 리 를 기준으로 파일구성한 경울보다 2배 이상 시간 단축



##### 결론
- 로우 기준으로 데이터를 분류할 수 있을때 partitioning 을 사용사면 성능 향상을 기대할 수 있다.

























