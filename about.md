---
layout: about
title: 오준혁
permalink: /about/
---

# About me
현재 Python과 Java 를 주력으로 만 2년차 백엔드 엔지니어로써 일하고 있습니다.

저는 개발자란 코딩을 하는 사람이 아닌 사용자가 원하는 데이터를 정확하고 신속하게 전달하는 방법을 탐구하고 개발해내는 사람이라 생각하고 있습니다

| github | [jheok](https://github.com/jheok), [jheok318](https://github.com/jheok318) |
|:-------|:---------------------------------------------------------------------------| 
| blog   | [jheok.github.io](https://jheok.github.io/about/)                          |

---
# Work Experience

## Mobigen (full-time employee)
> 웹 기반 빅데이터 분석 플랫폼 서비스

| period   | 21.01.01 ~ 현재 (2년 6개월) |
|:---------|:-----------------------|
| position | Python, Java 백엔드 엔지니어  |
| project  | 웹 백엔드 기반 데이터 분석 플랫폼 개발 |
{:.stretch-table}


### Data Fabric 플랫폼 개발 (23.06.01 ~ 현재, 1개월)

- 제안서를 통한 기획 준비 상태

[담당 역할]
- 기획


### sql+dsl 기반 데이터(polars+pandas) 분석 플랫폼 개발 (21.11.01 ~ 현재, 1년 8개월, GS 인증 진행중)

- 개발언어 : Python(FastApi, SQLAlchemy)
- 개발 참여인원: 2명
- 쿼리 토큰화, 파싱 및 RSA 암호화를 통한 데이터 보안 강화
- Pandas 와 Polars 결합으로 데이터 분석 성능 및 처리 속도 대폭 개선
- python GIL 로 인한 성능저하의 문제를 멀티 프로세싱으로 인한 개선
- Yacc(parser)와 Lex(tokenizer)를 이용하여 높은 성능의 DSL 개발
- Jenkins 를 활용하여 CI/CD 프로세스 구현
- 최적화된 릴리스 버전 외부 업체에 배포 완료, [예시 사이트](https://bigtori.kalis.or.kr)


[담당 역할]
- 기획, 설계, 개발

[성과 및 배운점]
- 데이터베이스에 직접 연결하여 2~3초 가량의 쿼리 처리를 수행하던 기존 서비스가 존재하였으며, 현 서비스로 인해 같은 쿼리를 1초이내로 줄여 새로운 백엔드 서비스로 교체
- 쿼리와 DSL 기반의 접근 방식을 도입하여, 기존의 복잡한 쿼리를 DSL 을 활용해 단순하고 가독성 높은 형태로 변환. 이를 통해 개발 및 유지 보수 과정에서의 효율성을 크게 향상

[해당 프로젝트 진행중 가장 기억에 남는 문제 해결 경험]
해당 프로젝트는 가상화된 데이터를 SQL+DSL 명령어를 통해 db 에 직접 연결하여 db 단에서 처리된 데이터를 추가적으로 사용자가 원하는 데이터로 가공하여 물리데이터로 변환해주는 프로젝트입니다.
프로젝트를 진행하면서 동시에 여러 쿼리가 동시에 서비스에 도착하면 성능 저하가 발생하는 문제를 발견하였습니다.
이를 ngrinder를 이용하여 재현하였고, Log 를 분석하여 Python의 GIL 때문에 성능이 저하되는 것으로 판단하였습니다.
이 문제를 해결하기 위해, 원래 하나의 프로세스로 운영되던 서비스를 5개의 다른 프로세스로 분리하여 성능 향상을 이루었습니다.
1. 메인 서버 (Main Server): API 요청을 처리하고 전반적인 서비스 관리를 담당합니다.
2. PreFork 서버 (PreFork Server): subprocess를 미리 fork 하여 db와 connection 을 미리 맺어놓고 SQL 쿼리를 처리합니다.
3. Fork 서버 (Fork Server): SQL+DSL 쿼리가 들어올 때마다 subprocess를 fork하며, 쿼리 처리 작업의 부하를 줄였습니다.
4. 모니터링 서버 (Monitoring Server): 각 subprocess의 상태를 모니터링하며, 특히 zombie process에 대한 관리를 하였습니다.
5. 로그 서버 (Log Server): 각 프로세스에서 발생하는 로그를 관리하고 여러 프로세스의 로그를 하나의 파일형태로 저장합니다.
   이러한 변경을 통해, 각 프로세스는 자신의 역할에 집중하고, 병목 현상을 최소화하여 성능을 향상을 하였습니다.
   추가로, ngrinder를 이용하여 성능 테스트를 수행하고, 이를 기반으로 서비스의 성능 개선을 지속적으로 모니터링하였습니다.
   이 결과, 원래 2~3초가 걸리던 쿼리 처리 시간을 1초 이하로 줄여, 성능을 2배 이상 향상시킬 수 있었습니다.
   이 경험을 통해, 성능 문제에 직면했을 때 시스템을 세분화하고, 특정 부분에 집중하여 문제를 해결하는 방법에 대해 배울 수 있었습니다.
   또한 성능 개선에 있어 지속적인 모니터링과 테스트의 중요성을 다시 한 번 인지하게 되었습니다.

### dsl 기반 빅데이터(spark) 분석 플랫폼 (레거시) 유지보수 (21.01.01 ~ 현재, 2년 6개월)

- 개발언어 : Python(flask, SQLAlchemy)
- 개발 참여인원: 3명
- 고성능 빅데이터 분석 서비스를 위한 Spark 활용
- 쿼리 토큰화, 파싱 및 암호화(RSA) 처리 최적화
- Spark 의 고성능 분석을 위한 최적화된 conf 설정 구축
- 로그 분석 용이성을 위한 Spark job 로그 세팅
- Spark 클러스터 서버 업그레이드 (1G -> 10G, 소프트웨어적 부분 담당)
- yacc(parser), lex(tokenizer)를 이용한 효율적인 DSL 개발
- 고성능 Pyspark UDF 개발
- 대용량 데이터 처리를 위한 파티셔닝 최적화 (느린 쿼리를 1초 내로 개선)
- Jenkins 를 통한 CI/CD 진행
- 외부 업체에 성능 향상된 릴리즈 버전 배포 완료

[담당 역할]
- 설계, 개발

[성과 및 배운점]
- 레거시 프로그램의 구성을 수정하여 기존 성능 대비 2배 이상의 성능 및 속도 향상
- 레거시 프로그램에 대한 깊이 있는 이해와 그에 접근하는 방법을 습득
- 다양한 개발자들과 함께 작업하는 프로젝트에서 GitFlow 를 사용한 경험을 통해 팀 협업에 대한 충분한 지식을 습득
- Spark 와 같은 데이터 분석 도구에 대한 개념을 이해하고, 데이터 분석에 필요한 다양한 도구를 활용할 수 있는 역량을 습득
- 데이터베이스에서 사용되는 쿼리의 구조 및 파싱에 대한 이해를 바탕으로, 독립적으로 쿼리를 구현할 수 있는 능력 습득

[해당 프로젝트 진행중 가장 기억에 남는 문제 해결 경험]
해당 프로젝트는 가상화된 데이터를 DSL 명령어를 통해 spark 를 통해 사용자가 원하는 데이터로 가공하여 물리데이터로 변환해주는 프로젝트이다.
그중 사용자가 등록한 데이터에 일치하는 폴리곤을 추출하여 사용자 데이터와 가상화된 데이터의 polygon 을 join 하여 다시 전달하는 명령어에서 문제가 발생했다.
이유는 가상화된 데이터는 100GB 의 .csv 파일을 단순 분할하여 데이터를 보유하고 있었기 때문이다.   
당시 해당 DSL 명령어는 무거운 폴리곤 데이터를 가지고 있기 때문에 조회를 하는게 당연히 느린 DSL 명령어라고 생각하여 단순 몇건에 대한 데이터에 polygon 데이터를 join을 해주는 명령어로써 사용되고 있었다.
하지만 나는 spark 를 사용하고 있는데 데이터 분석이 느린지에 대해서 이해를 할 수 없었다.
이를 해결하기 위해 raw 기반의 .csv가 아닌 column 기반의 parquet을 채택했다. 그리고 spark 에서 파티셔닝 된 parquet 파일을 읽을수 있다는 걸 확인했고 100GB 이상의 파일을 파티셔닝하는 과정을 거쳤다
이를 통해 사용자가 다량의 데이터를 등록할지라도 1초내로 결과를 얻어낼수 있게 작업을 했다.
이 경험을 통해 내가 처한 이슈를 해결하기 위해선 단순히 코드를 개발하거나 수정하는것 뿐만이 아닌 다른 여러가지 방법들로 사용자의 불편함을 해결할 수 있게 도와줄수 있구나를 배웠다.

### CI/CD 강화 (23.04.01 ~ 23.05.31, 2개월)

- 개발 참여인원: 4명
- GitLab, Jenkins, ArgoCD를 활용한 고성능 CI/CD 구축 (Jenkins 통합화 및 GitLab 모듈화 기여)
- 효율적인 리소스 활용을 위한 통합 Jenkinsfile 구축 (모든 repo 에서 하나의 Jenkinsfile 로 통합)
- Jenkins, SonarQube, Slack 연동을 통한 일일 빌드 결과 및 성능 분석 보고 개선

[담당 역할]
- 개발

[성과 및 배운점]
- 기존의 단순한 단위 테스트와 빌드 과정을 확장하여, 엔드 투 엔드 테스트와 배포까지 포괄하는 통합적인 CI/CD 파이프라인을 구축
- 모든 repo 에서 일관된 CI/CD 프로세스를 구현하여, 개발자들이 초기 Dockerfile 작성 및 간단한 설정만으로 손쉽게 CI/CD를 진행할 수 있도록 개선
- 좋은 개발자가 되기 위해서는 코드 작성 능력 뿐만 아니라, Jenkins 와 같은 도구를 능숙하게 사용하는 것이 중요하다는 것을 깨달음

### 메타데이터 관리 서비스 개선 (23.01.01 ~ 23.03.31, 3개월)

- 개발언어 : Java(Spring, JPA)
- 개발 참여인원: 2명
- Spring Datasource 에서 Flyway 를 이용한 DB 형상관리로 마이그레이션 (DB 히스토리 구축)
- JPA 기반 성능 향상으로 데이터 로딩 개선 및 로드 시간 단축
- Jenkins 를 활용한 CI/CD 진행
- 성능 개선된 릴리즈 버전 외부 업체 배포 완료

[담당 역할]
- 기획, 설계, 개발

[성과 및 배운점]
- 메타데이터 조회 과정을 개선하여, 전체 데이터 분석 후 가져오는 방식에서 특정 메타데이터만 가져오는 방식을 채택하여 성능을 1초에서 0.01초 수준으로 향상
- 다른 개발자가 작성한 코드에 성능 향상을 중점으로 리뷰함으로써, 현재 보유한 지식을 활용하여 프로그램이 현 상황에 적합한지 판단하고, 필요한 부분에 집중할 수 있는 시간을 가짐

[해당 프로젝트 진행중 가장 기억에 남는 문제 해결 경험]
해당 프로젝트는 가상화 된 데이터의 모든 메타데이터를 조회/생성/삭제/업데이트 하는 프로젝트이다
당시 가상화 데이터를 불러오는 서비스에서 메타데이터 관리 서비스를 거쳐 필요한 데이터를 가져와 물리적인 데이터로 변경을 하는 로직이 존재했는데
물리적인 데이터로 변환하는 부분이 아닌 메타데이터 관리 서비스를 거치는 부분에서 오버헤드가 발생하는 것이 모니터링 됐었다.
당시 메타데이터를 RDB 로 관리하다보니 정규화 과정을 거쳐 테이블이 세분화 되어있었다.
그러다보니 데이터의 조회를 하는 부분에서 메타데이터의 id 하나에 관련된 모든 테이블을 즉시로딩하여 데이터를 가지고 오고있었고 확인해보니 초창기에 비해 메타데이터의 양이 너무 비대하게 적재되어있었다.
이를 해결하기 위해 모든 데이터를 fetch 하는데 있어 지연로딩으로 변경하였고 N+1 문제가 발생되지않도록 fetch join 을 사용하였고 각 엔티티마다 persistalbe 을 상속받아 필요하지 않은 부분에서 merge() 가 실행되어 select 쿼리가 추가적으로 호출되는것을 방지하였다.
이로 인해 1초 대의 오버헤드가 0.01초 수준으로 내려갔다.
이 경험을 통해 spring을 spring 처럼 쓰는 방법을 배울수 있었다.

### 파일 기반 데이터 분석 플랫폼 개발 (22.07.01 ~ 22.12.31, 6개월, 환경공단 납품 완료)

- 개발언어 : Java(Spring, JPA), Python(FastApi, SQLAlchemy)
- 개발 참여인원: 1명
- 탐색적 데이터 분석(EDA)을 통한 시각화 데이터 추출 및 성능 향상
- 자연어 처리(NLP)를 이용한 분석 데이터 추출과 최적화
- GEO 폴리곤 데이터 추출 및 처리 성능 개선
- 잡스케줄러 구조 적용으로 서비스 안정성 향상
- Jenkins를 활용한 CI 진행
- 성능 개선된 릴리즈 버전 외부 업체 배포 완료

[담당 역할]
- 기획, 설계, 개발

[성과 및 배운점]
- 다수의 사용자가 하나의 서비스에 접속해 서버 과부하가 발생하는 문제를 해결하기 위해 서버를 분할
  이를 통해 트래픽과 분석 데이터의 크기가 증가해도 대용량 데이터 분석이 가능
- 개발 과정에서 언어는 단순한 도구임을 깨달음
- 로드 밸런싱을 구현해 기존 하나의 서비스를 여러 개의 서비스로 분할함으로써, 동기화 문제 등 새로운 이슈가 발생할 수 있다는 것을 인식

[해당 프로젝트 진행중 가장 기억에 남는 문제 해결 경험]
프로젝트의 주된 내용은 파일기반 분산 코드 실행 및 잡스케줄링 개발이었습니다. 이를 위해 Agent Manager Service(AMS)와 Agent Executor Service(AES)라는 두 개의 서비스를 개발했습니다
AMS는 Java로 개발되었으며, AES와 AES에서 생성된 Task(subprocess)를 모니터링하고 관리하는 잡스케줄러의 마스터 역할을 하였습니다
AES는 Python으로 개발되었으며, AMS를 통해 API 요청을 받으면 Task(subprocess)를 생성하여 분석을 하는 워커 역할을 하였습니다
프로젝트를 진행하면서 가장 기억에 남았던 문제는 동시성 문제였습니다. 초기에는 AMS와 AES, 각각의 서비스마다 pod을 하나씩 구동하면서 개발을 진행하였습니다. 이 때에는 동시성 문제가 발생하지 않았습니다.
그러나 프로젝트가 진행되면서 pod 관리를 자동화하는 단계로 넘어갔을 때 문제가 발생하였습니다. Kubernetes의 자동 스케일링 기능에 의해 AES의 pod 개수가 증가함에 따라, 여러 개의 pod이 동일한 파일에 동시에 액세스하는 상황이 발생하였습니다.
이러한 동시성 문제를 해결하기 위해 락을 도입하였습니다. 락을 이용하면 여러 개의 프로세스가 동시에 같은 자원에 접근하려고 할 때, 한 번에 하나의 프로세스만 접근할 수 있도록 제어할 수 있었습니다.
이 방법을 통해 동시성 문제를 해결했었습니다
이 경험을 통해 분산 시스템에서 동시성 문제는 불가피하며, 이를 처리하는 방법을 알아야 한다는 것을 깨달았습니다. 특히, 락을 적용하면서 동시성 제어의 중요성을 체감했습니다.

### slow 쿼리 캐시서버 개발 (21.06.01~21.12.31, 6개월)

- 개발언어 : Python(FastApi, SQLAlchemy)
- 개발 참여인원: 1명
- 쿼리 토큰화, 파싱 및 RSA 암호화 처리를 통한 성능 최적화
- 분석 플랫폼 내의 느린 쿼리 파악 후 캐싱 적용으로 성능 개선
- 응답 시간이 기존 10초에서 1초로 크게 단축된 성능 향상
- Jenkins를 이용한 CI/CD 진행
- 뚜렷한 성능 향상 덕분에 메인 프로젝트로 승격 및 진행 (다른 프로젝트로 인해 승격 프로젝트는 미진행)

[담당 역할]
- 기획, 설계, 개발

[성과 및 배운점]
- DB에서 데이터를 가져오고 전달하는 데 수십 초 소요되던 무거운 데이터를 캐시 서버를 활용하여 1초 이내로 처리할 수 있게 됨
- 데이터 서빙 시 빠른 처리와 전달만이 해결책이 아니라, 중간에 매개체를 도입함으로써 복잡함 대신 때로는 간결함이 효과적인 해결 방법이 될 수 있다는 것을 깨달음

---
## Mobigen (intern)

| period   | 20.07.01 ~ 20.12.31 (6개월)                                 |                                 
|:---------|:----------------------------------------------------------|
| position | Python 백엔드 엔지니어                                           |
| project  | 웹 백엔드 기반 데이터 분석 플랫폼 개발                                    |
| tech     | Python & flask, pySpark, MariaDB, Postgresql, docker, k8s |
{:.stretch-table}

### 자체 DSL 및 spark 기반 데이터 분석프로그램 분석 & 개발 (20.07.01 ~ 20.12.31, 6개월)
- 핵심 프로그램의 코드 분석과 성능 중심의 QA 진행
- 프로그램 동작 이해 후, 성능 향상을 위한 추가 DSL 개발 및 테스트 코드 작성
- 인턴 기간 종료 1개월 전에 성능 최적화된 프로그램을 배포하고 릴리즈 완료

[담당 역할]
- 기획, 설계, 개발, QA

---
# Experienced Skill
> 현재까지 제가 경험하고 접했던 기술들 입니다, 학부 과정에서의 경험은 포함하지 않았습니다

| Skill           |                                                                                    |
|:----------------|:-----------------------------------------------------------------------------------|
| 프로그래밍 언어        | Java, Python, Groovy                                                               |
| 프레임워크           | FastAPI, Flask, Spring                                                             |
| 프로토콜            | Http, Https, Grpc                                                                  |
| ORM 프레임워크       | JPA, SQLAlchemy                                                                    |
| 데이터베이스          | MariaDB, PostgreSQL, Redis, Memcached, Tibero, Oracle, Altibase, Minio, HDFS, 자체DB |
| DB 형상관리 툴       | Flyway, alembic                                                                    |
| 분산처리 엔진         | Spark                                                                              |
| 데이터 분석 툴        | Pandas, Polars, Pyspark, Apache Sedona(geoSpark)                                   |
| 자연어 분석 툴        | TextRank, Mecab                                                                    |
| 파싱 툴            | Yacc, Lex                                                                          |
| CI/CD           | Jenkins, ArgoCD                                                                    |
| 테스트 툴           | doctest, junit5, unittest, ngrinder                                                |
| 컨테이너화 및 오케스트레이션 | Docker, Kubernetes (k8s)                                                           |
| 코드 분석 툴         | Sonarqube, flake8                                                                  |
| 에러 트래킹          | Sentry                                                                             |
| 버전 관리 및 협업 도구   | GitHub, GitLab, Jira, Swagger, Mattermost, Slack                                   |
| CI/CD 언어        | Declarative pipeline, Scripted pipeline                                            |
{:.stretch-table}

---
## 학력

### 한양대 (erica)

| period | 19.03 ~ 21.02 |
|:-------|:--------------|
| 전공     | 컴퓨터 공학과       |
| 성적     | 3.65 / 4.5    |
| 졸업상태   | 졸업 / 편입       |
{:.stretch-table}

---
## 자격증

### AWS Certified Developer Associate

| 취득일   | 23.01.20   |
|:------|:-----------|
| 제공사   | AWS        |
| 성적    | 724 / 1000 |
{:.stretch-table}

